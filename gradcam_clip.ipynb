{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.7 64-bit ('sem_proj': conda)"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "gradcam_clip.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "afe03e785a399bd254041d765a8d7dc0796c9e91a37c1af437d36ceb61a03a67"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "source": [
        "import torch\n",
        "import clip\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt"
      ],
      "outputs": [],
      "metadata": {
        "id": "_A4lRslt2Anm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "source": [
        "\n",
        "def interpret_vit(image, text, model, device, index=None):\n",
        "    logits_per_image, logits_per_text = model(image, text)\n",
        "    probs = logits_per_image.softmax(dim=-1).detach().cpu().numpy()\n",
        "    if index is None:\n",
        "        index = np.argmax(logits_per_image.cpu().data.numpy(), axis=-1)\n",
        "    one_hot = np.zeros((1, logits_per_image.size()[-1]), dtype=np.float32)\n",
        "    one_hot[0, index] = 1\n",
        "    one_hot = torch.from_numpy(one_hot).requires_grad_(True)\n",
        "    one_hot = torch.sum(one_hot.cuda() * logits_per_image)\n",
        "    model.zero_grad()\n",
        "    one_hot.backward(retain_graph=True)\n",
        "\n",
        "    image_attn_blocks = list(dict(model.visual.transformer.resblocks.named_children()).values())\n",
        "    num_tokens = image_attn_blocks[0].attn_probs.shape[-1]\n",
        "    R = torch.eye(num_tokens, num_tokens, dtype=image_attn_blocks[0].attn_probs.dtype).to(device)\n",
        "    for blk in image_attn_blocks:\n",
        "        grad = blk.attn_grad\n",
        "        cam = blk.attn_probs\n",
        "        cam = cam.reshape(-1, cam.shape[-1], cam.shape[-1])\n",
        "        grad = grad.reshape(-1, grad.shape[-1], grad.shape[-1])\n",
        "        cam = grad * cam\n",
        "        cam = cam.clamp(min=0).mean(dim=0)\n",
        "        R += torch.matmul(cam, R)\n",
        "    R[0, 0] = 0\n",
        "    image_relevance = R[0, 1:]\n",
        "\n",
        "    # create heatmap from mask on image\n",
        "    def show_cam_on_image(img, mask):\n",
        "        heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
        "        heatmap = np.float32(heatmap) / 255\n",
        "        cam = heatmap + np.float32(img)\n",
        "        cam = cam / np.max(cam)\n",
        "        return cam\n",
        "\n",
        "    image_relevance = image_relevance.reshape(1, 1, 7, 7)\n",
        "    image_relevance = torch.nn.functional.interpolate(image_relevance, size=224, mode='bilinear')\n",
        "    image_relevance = image_relevance.reshape(224, 224).cuda().data.cpu().numpy()\n",
        "    image_relevance = (image_relevance - image_relevance.min()) / (image_relevance.max() - image_relevance.min())\n",
        "    image = image[0].permute(1, 2, 0).data.cpu().numpy()\n",
        "    image = (image - image.min()) / (image.max() - image.min())\n",
        "    vis = show_cam_on_image(image, image_relevance)\n",
        "    vis = np.uint8(255 * vis)\n",
        "    vis = cv2.cvtColor(np.array(vis), cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    plt.imshow(vis)\n",
        "#     plt.show()"
      ],
      "outputs": [],
      "metadata": {
        "id": "IFkchm5W2Ano"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "source": [
        "import streamlit as st\n",
        "\n",
        "from torchray.attribution.grad_cam import grad_cam\n",
        "import torch\n",
        "import clip\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "st.sidebar.header('Options')\n",
        "alpha = st.sidebar.radio(\"select alpha\", [0.5, 0.7, 0.8], index=1)\n",
        "layer = st.sidebar.selectbox(\"select saliency layer\", ['layer4.2.relu'], index=0)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_rn, preprocess = clip.load(\"RN50\", device=device, jit=False)\n",
        "\n",
        "def interpret_rn(image, text, model, device, index=None):   \n",
        "    image_features = model.encode_image(image)\n",
        "    text_features = model.encode_text(text)\n",
        "    image_features_norm = image_features.norm(dim=-1, keepdim=True)\n",
        "    image_features_new = image_features / image_features_norm\n",
        "    text_features_norm = text_features.norm(dim=-1, keepdim=True)\n",
        "    text_features_new = text_features / text_features_norm\n",
        "    logit_scale = model.logit_scale.exp()\n",
        "    logits_per_image = logit_scale * image_features_new @ text_features_new.t()\n",
        "    probs = logits_per_image.softmax(dim=-1).cpu().detach().numpy().tolist()\n",
        "    \n",
        "    text_prediction = (text_features_new * image_features_norm)\n",
        "    image_relevance = grad_cam(model.visual, image.type(model.dtype), text_prediction, saliency_layer=layer)\n",
        "        \n",
        "#     image_relevance = grad_cam(model.visual, image.type(model.dtype), image_features, saliency_layer=layer)\n",
        "\n",
        "    # create heatmap from mask on image\n",
        "    def show_cam_on_image(img, mask):\n",
        "        heatmap = cv2.applyColorMap(np.uint8(255 * mask), cv2.COLORMAP_JET)\n",
        "        heatmap = np.float32(heatmap) / 255\n",
        "        cam = heatmap + np.float32(img)\n",
        "        cam = cam / np.max(cam)\n",
        "        return cam\n",
        "\n",
        "    image_relevance = image_relevance.reshape(1, 1, 7, 7)\n",
        "    image_relevance = torch.nn.functional.interpolate(image_relevance, size=224, mode='bilinear')\n",
        "    image_relevance = image_relevance.reshape(224, 224).cuda().data.cpu().numpy()\n",
        "    image_relevance = (image_relevance - image_relevance.min()) / (image_relevance.max() - image_relevance.min())\n",
        "    image = image[0].permute(1, 2, 0).data.cpu().numpy()\n",
        "    image = (image - image.min()) / (image.max() - image.min())\n",
        "    vis = show_cam_on_image(image, image_relevance)\n",
        "    vis = np.uint8(255 * vis)\n",
        "    vis = cv2.cvtColor(np.array(vis), cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    plt.imshow(vis)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2021-09-27 13:58:35.719 \n",
            "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
            "  command:\n",
            "\n",
            "    streamlit run /home/mamooler/anaconda3/envs/sem_proj/lib/python3.9/site-packages/ipykernel_launcher.py [ARGUMENTS]\n",
            "100%|███████████████████████████████████████| 244M/244M [00:28<00:00, 9.04MiB/s]\n"
          ]
        }
      ],
      "metadata": {
        "id": "eUb_Rd5G2Anv",
        "outputId": "70082359-65a1-4a3f-f2ff-979157ae4425"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)"
      ],
      "outputs": [],
      "metadata": {
        "id": "Fjtms-ps2Anp",
        "outputId": "5ddd9d18-4863-4d55-dd7d-1ca22cfefc5a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "source": [
        "class color:\n",
        "   PURPLE = '\\033[95m'\n",
        "   CYAN = '\\033[96m'\n",
        "   DARKCYAN = '\\033[36m'\n",
        "   BLUE = '\\033[94m'\n",
        "   GREEN = '\\033[92m'\n",
        "   YELLOW = '\\033[93m'\n",
        "   RED = '\\033[91m'\n",
        "   BOLD = '\\033[1m'\n",
        "   UNDERLINE = '\\033[4m'\n",
        "   END = '\\033[0m'"
      ],
      "outputs": [],
      "metadata": {
        "id": "6O0lYX7N2Anq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "source": [
        "img_id = 'beach.jpg'\n",
        "MSCOCO_IMG_ROOT = \"\"\n",
        "\n",
        "# COCO_val2014_000000393267 What color is the woman's shirt on the left? {'black': 1, 'blonde': 0.3}\n",
        "import os\n",
        "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
        "ori_preprocess = Compose([\n",
        "        Resize((224), interpolation=Image.BICUBIC),\n",
        "    CenterCrop(size=(224, 224)),\n",
        "        ToTensor()])\n",
        "img_path = img_id # os.path.join(MSCOCO_IMG_ROOT, \"val2014\", img_id + \".jpg\")\n",
        "\n",
        "image = ori_preprocess(Image.open(img_id))\n",
        "print(preprocess)\n",
        "\n",
        "from matplotlib import rc\n",
        "\n",
        "font = {\n",
        "    'size': 32,\n",
        "}\n",
        "import matplotlib\n",
        "matplotlib.rcParams['mathtext.fontset'] = 'custom'\n",
        "matplotlib.rcParams['mathtext.rm'] = 'Bitstream Vera Sans'\n",
        "matplotlib.rcParams['mathtext.it'] = 'Bitstream Vera Sans:italic'\n",
        "matplotlib.rcParams['mathtext.bf'] = 'Bitstream Vera Sans:bold'\n",
        "# matplotlib.rcParams['mathtext.size'] = 16\n",
        "\n",
        "# {'cursive', 'fantasy', 'monospace', 'sans', 'sans serif', 'sans-serif', 'serif'}\n",
        "plt.figure(figsize=(16, 16))\n",
        "plt.tight_layout()\n",
        "plt.subplot(131)\n",
        "plt.imshow(image.permute(1, 2, 0))\n",
        "plt.axis('off')\n",
        "plt.title(\"(a) Original\", **font, y=-0.15)\n",
        "\n",
        "# plt.savefig('/rscratch/sheng.s/clip_boi/clip_vqa_starting/visual/sample_1_ori.pdf', bbox_inches='tight')\n",
        "# plt.show()\n",
        "\n",
        "image = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n",
        "texts = [\"Image of a beach\"]\n",
        "text = clip.tokenize(texts).to(device)\n",
        "print(color.BOLD + color.PURPLE + color.UNDERLINE + 'text: ' + texts[0] + color.END)\n",
        "plt.subplot(132)\n",
        "interpret_vit(model=model, image=image, text=text, device=device, index=0)\n",
        "plt.axis('off')\n",
        "plt.title(\"(b) ViT-B/32\", **font,y=-0.15)\n",
        "\n",
        "image = preprocess(Image.open(img_path)).unsqueeze(0).to(device)\n",
        "# texts = [\"What color is the woman's shirt on the left?\"]\n",
        "text = clip.tokenize(texts).to(device)\n",
        "plt.subplot(133)\n",
        "print(color.BOLD + color.PURPLE + color.UNDERLINE + 'text: ' + texts[0] + color.END)\n",
        "interpret_rn(model=model_rn, image=image, text=text, device=device, index=0)\n",
        "plt.axis('off')\n",
        "plt.title(\"(c) RN50\", **font,y=-0.15)\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.savefig('Grad_CAM_beach.pdf', bbox_inches='tight')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/home/mamooler/anaconda3/envs/sem_proj/lib/python3.9/site-packages/torchvision/transforms/transforms.py:280: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Compose(\n",
            "    Resize(size=224, interpolation=bicubic, max_size=None, antialias=None)\n",
            "    CenterCrop(size=(224, 224))\n",
            "    <function _convert_image_to_rgb at 0x7fe6156b0790>\n",
            "    ToTensor()\n",
            "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
            ")\n",
            "\u001b[1m\u001b[95m\u001b[4mtext: Image of a beach\u001b[0m\n"
          ]
        }
      ],
      "metadata": {
        "id": "SlhG0ekE2Anr",
        "outputId": "4733285c-f50b-4c6b-f6b1-2bbf51f8536c"
      }
    }
  ]
}